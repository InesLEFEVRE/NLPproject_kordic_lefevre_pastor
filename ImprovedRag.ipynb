{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import requests\n",
        "import shutil\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "# Download the dataset from Kaggle and load it into a pandas DataFrame\n",
        "dataset_path = \"books.csv\"\n",
        "df = pd.read_csv(dataset_path, on_bad_lines='skip')\n",
        "\n",
        "# Step 2: Data Preparation\n",
        "# Clean column names to remove any extra spaces\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Keep only essential fields for recommendations, if they exist in the dataset\n",
        "available_columns = df.columns.intersection(['bookID', 'title', 'authors', 'average_rating', 'language_code', 'num_pages', 'ratings_count', 'text_reviews_count'])\n",
        "df = df[available_columns]\n",
        "\n",
        "# Drop rows with missing values in essential fields\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Step 2.1: Create Descriptions for Each Book using Google Books API\n",
        "# Use Google Books API to fetch book descriptions\n",
        "API_KEY = 'AIzaSyD_BOUdSinR57PQWtiMO766HPVbbWM6PRA'\n",
        "\n",
        "def create_description_with_google_books_api(row):\n",
        "    book_title = row['title']\n",
        "    url = f\"https://www.googleapis.com/books/v1/volumes?q=intitle:{book_title}&key={API_KEY}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if \"items\" in data:\n",
        "            book_info = data['items'][0]['volumeInfo']\n",
        "            description = book_info.get('description', 'No description available')\n",
        "            return description\n",
        "    return 'No description available'\n",
        "\n",
        "df['description'] = df.apply(create_description_with_google_books_api, axis=1)\n",
        "\n",
        "# Step 3: Database Implementation using SQLite\n",
        "# Connect to SQLite database (or create it if it doesn't exist)\n",
        "db_path = '/content/books_recommendation.db'\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Step 4: Create a table for storing book information\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS books (\n",
        "    bookID INTEGER PRIMARY KEY,\n",
        "    title TEXT,\n",
        "    authors TEXT,\n",
        "    average_rating REAL,\n",
        "    language_code TEXT,\n",
        "    num_pages INTEGER,\n",
        "    ratings_count INTEGER,\n",
        "    text_reviews_count INTEGER,\n",
        "    description TEXT\n",
        ")\n",
        "''')\n",
        "\n",
        "# Step 5: Insert Data into SQLite Database\n",
        "for _, row in df.iterrows():\n",
        "    cursor.execute('''\n",
        "    INSERT INTO books (bookID, title, authors, average_rating, language_code, num_pages, ratings_count, text_reviews_count, description)\n",
        "    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "    ''', (\n",
        "        row.get('bookID'),\n",
        "        row.get('title'),\n",
        "        row.get('authors'),\n",
        "        row.get('average_rating'),\n",
        "        row.get('language_code'),\n",
        "        row.get('num_pages'),\n",
        "        row.get('ratings_count'),\n",
        "        row.get('text_reviews_count'),\n",
        "        row.get('description')\n",
        "    ))\n",
        "\n",
        "# Commit changes and close the connection\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"books.csv\"\n",
        "books_data = pd.read_csv(file_path)\n",
        "\n",
        "# Step 1: Data Cleaning\n",
        "# Drop rows where essential columns are missing\n",
        "essential_columns = ['title', 'authors', 'description', 'average_rating']\n",
        "books_data = books_data.dropna(subset=essential_columns)\n",
        "\n",
        "# Drop duplicates based on title and authors\n",
        "books_data = books_data.drop_duplicates(subset=['title', 'authors'])\n",
        "\n",
        "# Step 2: Text Cleaning\n",
        "def clean_text(text):\n",
        "    # Remove special characters, extra whitespace, and formatting artifacts\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.,!?'\\s]\", \" \", text)  # Keep alphanumeric, punctuation, and whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Clean relevant text fields\n",
        "books_data['title'] = books_data['title'].apply(clean_text)\n",
        "books_data['authors'] = books_data['authors'].apply(clean_text)\n",
        "books_data['description'] = books_data['description'].apply(clean_text)\n",
        "\n",
        "# Step 3: Combine Data for Retriever Corpus\n",
        "books_data['corpus'] = (\n",
        "    books_data['title'] + \" by \" + books_data['authors'] + \". \" + books_data['description']\n",
        ")\n",
        "\n",
        "# Step 4: Save Cleaned Data\n",
        "retriever_corpus_path = \"retriever_corpus_cleaned.txt\"\n",
        "books_data['corpus'].to_csv(retriever_corpus_path, index=False, header=False)\n",
        "\n",
        "cleaned_data_path = \"cleaned_books_data.csv\"\n",
        "books_data.to_csv(cleaned_data_path, index=False)\n",
        "\n",
        "print(f\"Data cleaning complete! Files saved:\\n- Retriever Corpus: {retriever_corpus_path}\\n- Cleaned Dataset: {cleaned_data_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RAG Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZccUupZj1ekx",
        "outputId": "25b87b0d-a349-4c4e-e25a-cbc7ee9ff546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in c:\\users\\ines\\miniforge3\\lib\\site-packages (1.9.0.post1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\ines\\miniforge3\\lib\\site-packages (from faiss-cpu) (2.2.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\ines\\miniforge3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qG2haRguJNc",
        "outputId": "85ab6cd2-149e-4789-c1b9-d8b3b9759e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUkGotwR1cM2",
        "outputId": "17123460-d4d6-44be-e67f-566566dc5e3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Ines\\miniforge3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\Ines\\miniforge3\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Ines\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Ines\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Ines\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Ines\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\Ines\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Ines\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from fuzzywuzzy import fuzz\n",
        "from fuzzywuzzy import process\n",
        "import re\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('wordnet')         # Download WordNet for lemmatization\n",
        "nltk.download('omw-1.4')         # Optional: WordNet dependencies for multilingual support\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ErFf3Q1ONU",
        "outputId": "3be6c33c-b8fc-4047-a8c1-7f2640c984a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "User Query: I want to read books about magic and adventure.\n",
            "Parsed Preferences: {'keywords': ['book', 'magic', 'adventure'], 'genre': ['fantasy'], 'author': []}\n",
            "\n",
            "Generated Response:\n",
            "Here are 3 book recommendations for you:\n",
            "\n",
            "1. **Title:** Haunted Castle on Hallows Eve Magic Tree House 30\n",
            "   **Author:** Mary Pope Osborne Salvatore Murdocca\n",
            "   **Description:** The 1 bestselling chapter book series of all time celebrates 25 years with new covers and a new, easy to use numbering system! Jack and Annie are summoned once again to the fantasy realm of Camelot. T...\n",
            "\n",
            "2. **Title:** Magician Apprentice The Riftwar Saga 1\n",
            "   **Author:** Raymond E. Feist Brett Booth\n",
            "   **Description:** The Riftwar Saga a classic of fantasy literature which no true fan should be without opens with this tale of magic, might, and adventure. One of the world s most successful fantasy fiction authors. Th...\n",
            "\n",
            "3. **Title:** Owlknight Owl Mage Trilogy 3\n",
            "   **Author:** Mercedes Lackey Larry Dixon\n",
            "   **Description:** Following on from OWLFLIGHT and OWLSIGHT, Our Hero Darian has just passed the tests to become Master Mage of the Vale. He's been made a Knight of Valdemar and a Clanbrother. But a new Herald Mage is a...\n",
            "\n",
            "User query: I want to read books about magic and adventure. Here are some book recommendations based on your query. Please explain why these books might be a good fit for the user. The Riftwar Saga is a classic of fantasy literature which no true fan should be without. Owlknight Owl Mage Trilogy follows on from OWLFLIGHT and OWLSIGHT.\n",
            "--------------------------------------------------\n",
            "\n",
            "User Query: Looking for romantic novels.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ines\\AppData\\Local\\Temp\\ipykernel_8284\\1947915039.py:82: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
            "  similarity = np.dot(query_embedding, genre_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(genre_embedding))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsed Preferences: {'keywords': ['romantic', 'novel'], 'genre': ['romance', 'classics'], 'author': []}\n",
            "\n",
            "Generated Response:\n",
            "Here are 3 book recommendations for you:\n",
            "\n",
            "1. **Title:** Love Artist Harlequin Romance 2860\n",
            "   **Author:** Valerie Parv\n",
            "   **Description:** Swashbuckling sailors, dashing dukes, naughty nurses, and sexy steward esses caught in webs of love, passion, betrayal, and intrigue these are the raw materials of the romance novel and the lusty cove...\n",
            "\n",
            "2. **Title:** Memory's Embrace Corbins 3\n",
            "   **Author:** Linda Lael Miller\n",
            "   **Description:** A classic Western romance from beloved 1 New York Times bestselling author Linda Lael Miller. In the wilderness of 1880s Oregon, beautiful Tess Bishop was captivated by the most fascinating stranger e...\n",
            "\n",
            "3. **Title:** Little Women\n",
            "   **Author:** Louisa May Alcott Jessie Willcox Smith Frank T. Merrill\n",
            "   **Description:** One of the best loved books of all time. Nominated as one of America s best loved novels by PBS s The Great American Read Lovely Meg, talented Jo, frail Beth, spoiled Amy these are hard lessons of pov...\n",
            "\n",
            "User query: Looking for romantic novels. Here are some book recommendations based on your query. Please explain why these books might be a good fit for the user. For confidential support call the Samaritans on 08457 90 90 90 or visit a local Samaritans branch, see www.samaritans.org for details.\n",
            "--------------------------------------------------\n",
            "\n",
            "User Query: Recommend books by Tolkien about fantasy.\n",
            "Parsed Preferences: {'keywords': ['recommend', 'book', 'tolkien', 'fantasy'], 'genre': ['fantasy'], 'author': 'J.R.R. Tolkien'}\n",
            "\n",
            "Generated Response:\n",
            "Here are 3 book recommendations for you:\n",
            "\n",
            "1. **Title:** The Lord of the Rings The Lord of the Rings 1 3\n",
            "   **Author:** J.R.R. Tolkien\n",
            "   **Description:** A beautiful illustrated boxed set collecting the two most popular Tolkien hardbacks the Centenary edition of The Lord of the Rings and the 60th Anniversary edition of The Hobbit, both illustrated by A...\n",
            "\n",
            "2. **Title:** Morgoth's Ring The History of Middle Earth 10\n",
            "   **Author:** J.R.R. Tolkien Christopher Tolkien\n",
            "   **Description:** Fourth in a series of hardcover box sets celebrating the literary achievement of Christopher Tolkien, featuring double sided dust jackets one side featuring artwork by John Howe, and the original grap...\n",
            "\n",
            "3. **Title:** The Lost Road and Other Writings The History of Middle earth 5\n",
            "   **Author:** J.R.R. Tolkien Christopher Tolkien\n",
            "   **Description:** Second in a series of hardcover box sets celebrating the literary achievement of Christopher Tolkien, featuring double sided dust jackets. Set 2 contains The Lays of Beleriand, The Shaping of Middle e...\n",
            "\n",
            "User query: Recommend books by Tolkien about fantasy. Here are some book recommendations based on your query. Please explain why these books might be a good fit for the user. The Lord of the Rings, The History of Middle Earth and The Lost Road all feature in this list.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Load your dataset\n",
        "file_path = \"cleaned_books_data.csv\"\n",
        "books_data = pd.read_csv(file_path)\n",
        "df=books_data\n",
        "\n",
        "# ----------------------------\n",
        "# 1. User Input Processing\n",
        "# ----------------------------\n",
        "\n",
        "def process_user_input(user_query):\n",
        "    \"\"\"\n",
        "    Parse user input to extract preferences like keywords, genre, and author.\n",
        "    \"\"\"\n",
        "    preferences = {\"keywords\": [], \"genre\": [], \"author\": []}\n",
        "\n",
        "    # Tokenize the query and apply POS tagging\n",
        "    tokens = word_tokenize(user_query.lower())\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Stopwords for filtering\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatize and extract meaningful words (nouns and adjectives)\n",
        "    keywords = [\n",
        "        lemmatizer.lemmatize(word) for word, pos in pos_tags\n",
        "        if pos in [\"NN\", \"NNS\", \"JJ\", \"NNP\", \"NNPS\"] and word not in stop_words  # Include proper nouns (NNP, NNPS)\n",
        "    ]\n",
        "\n",
        "    preferences[\"keywords\"] = keywords\n",
        "\n",
        "    # Assign genres or authors based on keywords\n",
        "    predefined_keywords = {\n",
        "        \"fantasy\": {\"magic\", \"adventure\", \"fantasy\", \"wizard\", \"quest\", \"epic\", \"mystery\", \"dark\", \"tale\", \"story\"},\n",
        "        \"romance\": {\"love\", \"heart\", \"romance\", \"relationships\", \"passion\", \"affection\", \"wedding\", \"desire\"},\n",
        "        \"science fiction\": {\"fiction\", \"sci-fi\", \"space\", \"future\", \"technology\", \"alien\", \"robot\", \"dystopia\", \"cyberpunk\"},\n",
        "        \"historical fiction\": {\"history\", \"past\", \"classic\", \"war\", \"empire\", \"revolution\", \"king\", \"queen\", \"ancient\"},\n",
        "        \"mystery\": {\"murder\", \"mystery\", \"dark\", \"secret\", \"death\", \"detective\", \"crime\", \"thriller\", \"suspense\", \"investigation\"},\n",
        "        \"young adult\": {\"young\", \"teen\", \"friends\", \"school\", \"coming-of-age\", \"life\", \"journey\", \"youth\", \"high school\"},\n",
        "        \"classics\": {\"classic\", \"timeless\", \"literature\", \"award\", \"masterpiece\", \"beloved\", \"century\", \"english\"},\n",
        "        \"horror\": {\"horror\", \"scary\", \"ghost\", \"dark\", \"haunted\", \"fear\", \"evil\", \"nightmare\", \"supernatural\"},\n",
        "        \"family\": {\"family\", \"father\", \"mother\", \"relationships\", \"home\", \"life\", \"children\", \"heart\", \"love\"},\n",
        "        \"non-fiction\": {\"history\", \"biography\", \"memoir\", \"education\", \"real\", \"truth\", \"facts\", \"science\", \"philosophy\", \"politics\"},\n",
        "        }\n",
        "\n",
        "    matched_genres = set()\n",
        "    for genre, keywords_set in predefined_keywords.items():\n",
        "        if any(keyword in keywords_set for keyword in keywords):\n",
        "            matched_genres.add(genre)\n",
        "\n",
        "    preferences[\"genre\"] = list(matched_genres)\n",
        "\n",
        "    # If no genres matched, use semantic similarity as fallback\n",
        "    if not preferences[\"genre\"]:\n",
        "        preferences[\"genre\"] = associate_genre_semantically(user_query, predefined_keywords)\n",
        "\n",
        "\n",
        "    # Extract author using a simple pattern\n",
        "    author_match = re.search(r\"by\\s+([a-zA-Z\\s]+)\", user_query)\n",
        "    if author_match:\n",
        "        preferences[\"author\"] = author_match.group(1).strip()\n",
        "\n",
        "    matched_author = identify_author_from_db(preferences['keywords'], df)\n",
        "    if matched_author:\n",
        "        preferences[\"author\"] = matched_author\n",
        "\n",
        "    print(\"Parsed Preferences:\", preferences)  # Debugging\n",
        "    return preferences\n",
        "\n",
        "\n",
        "def associate_genre_semantically(user_query, predefined_keywords):\n",
        "    \"\"\"\n",
        "    Associate genres with the query using semantic similarity.\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    query_embedding = model.encode(user_query, convert_to_tensor=True)\n",
        "\n",
        "    genre_scores = {}\n",
        "    for genre, keywords in predefined_keywords.items():\n",
        "        genre_keywords = \" \".join(keywords)\n",
        "        genre_embedding = model.encode(genre_keywords, convert_to_tensor=True)\n",
        "        similarity = np.dot(query_embedding, genre_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(genre_embedding))\n",
        "        genre_scores[genre] = similarity\n",
        "\n",
        "    # Select genres with similarity above a threshold\n",
        "    selected_genres = [genre for genre, score in genre_scores.items() if score > 0.3]\n",
        "    return selected_genres\n",
        "\n",
        "def identify_author_from_db(keywords, df):\n",
        "    \"\"\"\n",
        "    Identify an author from a DataFrame based on fuzzy matching with keywords.\n",
        "\n",
        "    Args:\n",
        "        keywords (list): List of keywords or input words.\n",
        "        df (DataFrame): Book database with an 'authors' column.\n",
        "\n",
        "    Returns:\n",
        "        str: Name of the matched author or None if no match is found.\n",
        "    \"\"\"\n",
        "    if not keywords:  # Return None immediately if keywords is empty\n",
        "        return None\n",
        "\n",
        "    # Remove generic or stop-like words from keywords\n",
        "    irrelevant_keywords = {\"book\", \"not\", \"similar\", \"read\", \"recommend\"}\n",
        "    keywords = [word.lower().strip() for word in keywords if word not in irrelevant_keywords]\n",
        "\n",
        "    if not keywords: # No keywords left after filtering\n",
        "        return None\n",
        "\n",
        "    best_match = None\n",
        "    highest_score = 0\n",
        "\n",
        "    # Clean up author names in the DataFrame\n",
        "    authors = df['authors'].dropna().unique()\n",
        "    authors = [author.strip() for author in authors]\n",
        "\n",
        "    # Compare each author with query keywords using fuzzy matching\n",
        "    for author in authors:\n",
        "        for keyword in keywords:\n",
        "            # Match only if the keyword is at least 3 characters long\n",
        "            if len(keyword) >= 3:\n",
        "                score = fuzz.partial_ratio(author.lower(), keyword)\n",
        "                if score > highest_score and score > 90:  # Stricter threshold\n",
        "                    # Ensure that the matched author has at least two words (e.g., \"First Last\")\n",
        "                    if len(author.split()) >= 2:\n",
        "                        best_match = author\n",
        "                        highest_score = score\n",
        "\n",
        "    return best_match\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Retrieval Mechanism\n",
        "# ----------------------------\n",
        "\n",
        "def retrieve_books(preferences, retriever_model, index, df, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve the top relevant books using a retriever model and FAISS, with additional filtering.\n",
        "    \"\"\"\n",
        "    # Filter the DataFrame based on genre and author\n",
        "    filtered_df = df.copy()\n",
        "    if preferences['genre']:\n",
        "        genre_filter = \"|\".join(preferences['genre'])\n",
        "        filtered_df = filtered_df[filtered_df['description'].str.contains(genre_filter, case=False, na=False)]\n",
        "    if preferences['author']:\n",
        "        filtered_df = filtered_df[filtered_df['authors'].str.contains(preferences['author'], case=False, na=False)]\n",
        "\n",
        "    # Drop exact duplicate rows after filtering\n",
        "    filtered_df = filtered_df.drop_duplicates(subset=['title', 'authors', 'description'])\n",
        "\n",
        "    # If no filters apply, fallback to full dataset for query-based retrieval\n",
        "    if filtered_df.empty:\n",
        "        filtered_df = df.copy()\n",
        "        print(\"No books match the specified filters. Using all books for retrieval.\")\n",
        "\n",
        "    # Construct a robust query string\n",
        "    query_parts = preferences['keywords'] + preferences['genre']\n",
        "    if preferences['author']:\n",
        "        query_parts.insert(0, preferences['author'])  # Prioritize author keywords in query\n",
        "    query_string = \" \".join(query_parts).strip()\n",
        "    query_embedding = retriever_model.encode([query_string], convert_to_tensor=False)\n",
        "    query_embedding = np.array(query_embedding).astype('float32').reshape(1, -1)\n",
        "\n",
        "    # Build FAISS index for the filtered results\n",
        "    filtered_embeddings = retriever_model.encode(filtered_df['description'].tolist(), convert_to_tensor=False)\n",
        "    filtered_embeddings = np.array(filtered_embeddings).astype('float32')\n",
        "    index = faiss.IndexFlatL2(filtered_embeddings.shape[1])\n",
        "    index.add(filtered_embeddings)\n",
        "\n",
        "    # Retrieve more results initially to ensure sufficient relevant books\n",
        "    num_results = min(top_k * 5, len(filtered_df))  # Retrieve extra candidates\n",
        "    distances, indices = index.search(query_embedding, num_results)\n",
        "\n",
        "    # Retrieve the corresponding rows from filtered DataFrame\n",
        "    filtered_indices = filtered_df.index[indices[0]].tolist()\n",
        "    retrieved_books = filtered_df.loc[filtered_indices, ['title', 'authors', 'description']]\n",
        "\n",
        "    # Prioritize books by the detected author (if any)\n",
        "    if preferences['author']:\n",
        "        retrieved_books['priority'] = retrieved_books['authors'].apply(\n",
        "            lambda x: 0 if preferences['author'].lower() in x.lower() else 1\n",
        "        )\n",
        "        retrieved_books = retrieved_books.sort_values(by='priority')\n",
        "\n",
        "    # Drop duplicates on title level and limit to top_k results\n",
        "    retrieved_books = retrieved_books.drop_duplicates(subset=['title']).head(top_k)\n",
        "\n",
        "    return retrieved_books[['title', 'authors', 'description']].to_dict(orient='records')\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Response Generation\n",
        "# ----------------------------\n",
        "\n",
        "def generate_response(retrieved_books, user_query):\n",
        "    \"\"\"\n",
        "    Generate a natural language response suggesting books.\n",
        "\n",
        "    Args:\n",
        "        retrieved_books (list): List of retrieved books (formatted).\n",
        "        user_query (str): The user's original query.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with book recommendations and a natural language response.\n",
        "    \"\"\"\n",
        "    # Load the pretrained generative model (BART or GPT)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "    # Handle empty retrieval results\n",
        "    if not retrieved_books:\n",
        "        return {\"generated_response\": \"Sorry, no relevant books were found for your query.\", \"books\": []}\n",
        "\n",
        "    # Format the retrieved books into a clean list\n",
        "    book_details = \"\\n\\n\".join(\n",
        "        [f\"{i+1}. **Title:** {book['title']}\\n   **Author:** {book['authors']}\\n   **Description:** {book['description'][:200]}...\"\n",
        "         for i, book in enumerate(retrieved_books[:3])]\n",
        "    )\n",
        "\n",
        "    # Create a natural language generation prompt\n",
        "    prompt = (\n",
        "        f\"User query: {user_query}\\n\\n\"\n",
        "        f\"Here are some book recommendations based on your query:\\n\\n\"\n",
        "        f\"{book_details}\"\n",
        "        f\"Please explain why these books might be a good fit for the user.\"\n",
        "    )\n",
        "\n",
        "    # Debugging: Print the prompt for verification\n",
        "    # print(\"\\nGenerated Prompt:\")\n",
        "    # print(prompt)\n",
        "\n",
        "    # Tokenize input and generate response\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=300,\n",
        "        num_beams=5,\n",
        "        repetition_penalty=2.5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Combine the generated response and formatted book details\n",
        "    formatted_response = f\"Here are 3 book recommendations for you:\\n\\n{book_details}\\n\\n{response}\"\n",
        "    return formatted_response\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Full Pipeline: Recommend Books\n",
        "# ----------------------------\n",
        "def recommend_books(user_query, retriever_model, index, df):\n",
        "    \"\"\"\n",
        "    End-to-end function to recommend books based on user input.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): User's query for book recommendations.\n",
        "        retriever_model: Sentence embedding model for retrieval.\n",
        "        index: FAISS index for fast semantic search.\n",
        "        df (DataFrame): Book dataset.\n",
        "\n",
        "    Returns:\n",
        "        str: A natural language response with recommended books.\n",
        "    \"\"\"\n",
        "    # User Input Processing\n",
        "    preferences = process_user_input(user_query)\n",
        "\n",
        "    # Retrieval Mechanism\n",
        "    retrieved_books = retrieve_books(preferences, retriever_model, index, df)\n",
        "\n",
        "    # Handle empty results\n",
        "    if not retrieved_books:\n",
        "        return \"Sorry, I couldn't find any books matching your preferences.\"\n",
        "\n",
        "    # Response Generation\n",
        "    response = generate_response(retrieved_books, user_query)\n",
        "    return response\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Example Execution\n",
        "# ----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Initialize retriever model and FAISS index\n",
        "    retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    embeddings = retriever_model.encode(df['description'].tolist(), convert_to_tensor=False)\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    # Test the system\n",
        "    test_queries = [\n",
        "        \"I want to read books about magic and adventure.\",\n",
        "        \"Looking for romantic novels.\",\n",
        "        \"Recommend books by Tolkien about fantasy.\"\n",
        "    ]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\nUser Query: {query}\")\n",
        "        response = recommend_books(query, retriever_model, index, df)\n",
        "        print(\"\\nGenerated Response:\")\n",
        "        print(response)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "def recommend_books_rag(user_query):\n",
        "    return recommend_books(user_query,retriever_model, index,df=books_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#################################  CHATBOT   #############################################\n",
        "\n",
        "\n",
        "from flask import Flask, request, render_template, jsonify\n",
        "import re\n",
        "\n",
        "\n",
        "# Flask app initialization\n",
        "app = Flask(__name__)\n",
        "\n",
        "\n",
        "# Flask routes\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/recommend', methods=['POST'])\n",
        "def recommend():\n",
        "    data = request.get_json()  # Parse JSON input\n",
        "    user_query = data.get('query')  # Extract query from input\n",
        "    if not user_query:\n",
        "        return jsonify({\"error\": \"No query provided!\"}), 400\n",
        "    response = recommend_books_rag(user_query)  # Get book recommendations\n",
        "    return jsonify({\"recommendations\": response})  # Return JSON response\n",
        "\n",
        "if __name__ == '_main_':\n",
        "    app.run(debug=True, port=5001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting flask\n",
            "  Downloading flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting Werkzeug>=3.1 (from flask)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\ines\\miniforge3\\lib\\site-packages (from flask) (3.1.4)\n",
            "Collecting itsdangerous>=2.2 (from flask)\n",
            "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: click>=8.1.3 in c:\\users\\ines\\miniforge3\\lib\\site-packages (from flask) (8.1.7)\n",
            "Collecting blinker>=1.9 (from flask)\n",
            "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\ines\\miniforge3\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ines\\miniforge3\\lib\\site-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Downloading flask-3.1.0-py3-none-any.whl (102 kB)\n",
            "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
            "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Installing collected packages: Werkzeug, itsdangerous, blinker, flask\n",
            "Successfully installed Werkzeug-3.1.3 blinker-1.9.0 flask-3.1.0 itsdangerous-2.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install flask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name '_name_' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Flask app initialization\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m app \u001b[38;5;241m=\u001b[39m Flask(\u001b[43m_name_\u001b[49m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Flask routes\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;129m@app\u001b[39m\u001b[38;5;241m.\u001b[39mroute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mindex\u001b[39m():\n",
            "\u001b[1;31mNameError\u001b[0m: name '_name_' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
